{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9517c26a-cecc-4363-97d4-4538aeacb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/vol/tensusers5/nhollain/s_clip_scripts')\n",
    "\n",
    "import itertools\n",
    "from main import main, format_checkpoint\n",
    "from params import parse_args\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prep_str_args(str_args): # Code to parse the string style arguments, as shown below\n",
    "    str_args = str_args.split('\\n') # Split on newline\n",
    "    str_args = [s.strip() for s in str_args] # Remove any whitespaces from the start and end of the strings\n",
    "    # Split on the space between the parameter name and the value, e.g. '--name x' becomes ['--name', 'x']\n",
    "    str_args = [s.split(' ') for s in str_args] \n",
    "    str_args = list(itertools.chain(*str_args)) # Flatten the resulting list of lists\n",
    "    str_args = [s for s in str_args if len(s) > 0] # Remove arguments that are empty\n",
    "    return str_args\n",
    "    \n",
    "def evaluate(checkpoint, epoch = None, kfold = -1):\n",
    "    print('=> Resuming checkpoint {} (epoch {})'.format(checkpoint, epoch))\n",
    "    checkpoint = checkpoint_path \n",
    "    if 'Fashion' in checkpoint:\n",
    "        zeroshot_datasets = [\"Fashion200k-SUBCLS\", \"Fashion200k-CLS\", \"FashionGen-CLS\", \"FashionGen-SUBCLS\", \"Polyvore-CLS\", ]\n",
    "        retrieval_datasets = [\"FashionGen\", \"Polyvore\", \"Fashion200k\",]\n",
    "    else:\n",
    "        zeroshot_datasets = [\"RSICD-CLS\", \"UCM-CLS\"] # \"WHU-RS19\", \"RSSCN7\", \"AID\" -> NOT WORKING bc of different data-loading workings\n",
    "        retrieval_datasets = [\"RSICD\", \"UCM\", \"Sydney\"]\n",
    "    \n",
    "    for dataset in zeroshot_datasets:\n",
    "        str_args = ['--name', checkpoint, '--imagenet-val', dataset, '--resume-epoch', str(epoch), 'k-fold', str(kfold)]\n",
    "        args = parse_args(str_args)\n",
    "        main(args)\n",
    "    \n",
    "    for dataset in retrieval_datasets:\n",
    "        str_args = ['--name', checkpoint, '--val-data', dataset, '--resume-epoch', str(epoch)]\n",
    "        args = parse_args(str_args)\n",
    "        main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99e8109-1b98-4d55-96b6-ba9d06db445b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_RS-ALL-ratio_0.1-model_RN50-method_base-kw_none-AL_False-PL_None-vit_False-epochs_10-lr_0.0005-bs_64': 80}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "results = []\n",
    "if os.path.exists('./eval.txt'):   \n",
    "    with open('./eval.txt', 'r') as f:\n",
    "        results = f.readlines()\n",
    "\n",
    "# Remove any non-result lines from the eval file, and split the lines on the tab character\n",
    "# (results have format: model_name\\tdataset_name\\tmetric_name\\tmetric_value)\n",
    "results = [r.replace('\\n','').split('\\t')[0] for r in results if '\\t' in r]\n",
    "model_names = results\n",
    "# Remove the timestamp from the model names, as well as the specific fold - rest of the name contains params\n",
    "model_names = ['-'.join(m.split('-')[2:]).split('-fold')[0] for m in model_names]\n",
    "model_names = dict(Counter(model_names))\n",
    "# Show which models are already present in eval.txt\n",
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6311e1-c938-434d-9d14-04385c1347b0",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7eaa23-316d-4c09-847e-bdd9c30550a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of configs: 3\n"
     ]
    }
   ],
   "source": [
    "# Do a grid search on the parameters\n",
    "base_str_args = ''' --train-data RS-ALL\n",
    "--label-ratio 0.1\n",
    "--val-data RS-ALL\n",
    "--imagenet-val RSICD-CLS \n",
    "--keyword-path keywords/RS/class-name.txt\n",
    "--zeroshot-frequency 5  \n",
    "--method base\n",
    "--save-freq 5\n",
    "'''\n",
    "\n",
    "# Dictionary of values to gridsearch for hyperparam tuning\n",
    "gridsearch_dict = {\n",
    "    '--epochs' : [10], #[10,15,20,25,30,35],\n",
    "    '--lr' : [5e-4, 5e-5, 5e-6], # 5e-4, 5e-6\n",
    "    '--batch-size' : [64] #,128,256],\n",
    "}\n",
    "\n",
    "num_repeats = 9\n",
    "num_evals = 20 # How many evaluations are done with evaluate(...)\n",
    "\n",
    "gridsearch_values = list(gridsearch_dict.values())\n",
    "gridsearch_keys = list(gridsearch_dict.keys())\n",
    "configs = list(itertools.product(*gridsearch_values))\n",
    "print('Number of configs:', len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f405aaa0-49e5-47ad-9ec2-0dadb0ae4522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--train-data', 'RS-ALL', '--label-ratio', '0.1', '--val-data', 'RS-ALL', '--imagenet-val', 'RSICD-CLS', '--keyword-path', 'keywords/RS/class-name.txt', '--zeroshot-frequency', '5', '--method', 'base', '--save-freq', '5', '--epochs', '10', '--lr', '0.0005', '--batch-size', '64']\n",
      "Config number 0: 5 repeats\n",
      "\n",
      "repeat number 4\n",
      "Epochs to checkpoint [5, 10]\n",
      "formatting...\n",
      "kfold: 4\n",
      "Log path: ./checkpoint/2023_11_09-10_09_28-data_RS-ALL-ratio_0.1-model_RN50-method_base-kw_none-AL_False-PL_None-vit_False-epochs_10-lr_0.0005-bs_64-fold_4\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.58 GiB total capacity; 503.11 MiB already allocated; 19.62 MiB free; 526.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:32\u001b[0m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/s_clip_scripts/main.py:167\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    162\u001b[0m random_seed(args\u001b[38;5;241m.\u001b[39mseed, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    163\u001b[0m model, preprocess_train, preprocess_val \u001b[38;5;241m=\u001b[39m create_model_and_transforms(\n\u001b[1;32m    164\u001b[0m     args\u001b[38;5;241m.\u001b[39mmodel, args\u001b[38;5;241m.\u001b[39mpretrained, precision\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mprecision, device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    165\u001b[0m     aug_cfg \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39maug_cfg, )\n\u001b[0;32m--> 167\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_custom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# use custom model\u001b[39;00m\n\u001b[1;32m    169\u001b[0m random_seed(args\u001b[38;5;241m.\u001b[39mseed, args\u001b[38;5;241m.\u001b[39mrank)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtrain_data: \u001b[38;5;66;03m# Keep track of the parameters of the model using params.txt\u001b[39;00m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/s_clip_scripts/model.py:8\u001b[0m, in \u001b[0;36mcreate_custom_model\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_custom_model\u001b[39m(args, model):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCustomCLIP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_vit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/s_clip_scripts/model.py:20\u001b[0m, in \u001b[0;36mCustomCLIP.__init__\u001b[0;34m(self, clip, device, use_vit)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_processor \u001b[38;5;241m=\u001b[39m AutoImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_ckpt)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_vit \u001b[38;5;241m=\u001b[39m use_vit\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstored_vit \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2058\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2056\u001b[0m     )\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.58 GiB total capacity; 503.11 MiB already allocated; 19.62 MiB free; 526.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for c, config in enumerate(configs): # Gridsearch\n",
    "    str_args = copy.deepcopy(base_str_args)\n",
    "    # Add the gridsearch parameters to the arguments\n",
    "    for i, param in enumerate(config):\n",
    "        param_name = gridsearch_keys[i]\n",
    "        str_args += '\\n{} {}'.format(param_name, param)\n",
    "        \n",
    "    str_args = prep_str_args(str_args)\n",
    "    print(str_args)\n",
    "    args = parse_args(str_args)\n",
    "    checkpoint_hypothetical = format_checkpoint(args)\n",
    "    # Remove the timestamp from the hypothetical checkpoint, so we can compare to the params of other checkpoints\n",
    "    checkpoint_params = '-'.join(checkpoint_hypothetical.split('-')[2:]).split('-fold')[0]\n",
    "    # print(checkpoint_params)\n",
    "    # Check if we've already trained the exact same model, correct the number of training iterations we still need to do\n",
    "    if checkpoint_params in model_names:\n",
    "        # The number of times to repeat depends on how often the model's been evaluated already\n",
    "        start_repeat = int(model_names[checkpoint_params]/num_evals)\n",
    "    else: # If we've never trained + evaluated the model before, just use num_repeats\n",
    "        start_repeat = 0\n",
    "    print('Config number {}: {} repeats'.format(c, num_repeats-start_repeat))\n",
    "    for i in range(start_repeat, num_repeats):\n",
    "        print()\n",
    "        print('repeat number', i) \n",
    "        args = parse_args(str_args)\n",
    "        args.k_fold = i\n",
    "        # We compute here for which epochs we need to evaluate (based on for which epochs we checkpoint)\n",
    "        epoch_freq = args.save_freq\n",
    "        epochs = list(range(epoch_freq,args.epochs+1,epoch_freq))\n",
    "        print('Epochs to checkpoint', epochs)\n",
    "        # print('Args k fold (outside):' , args.k_fold)\n",
    "        checkpoint_path = main(args) # Calls the main.py function of S-CLIP\n",
    "        for epoch in epochs:\n",
    "            evaluate(checkpoint_path, epoch = epoch, kfold = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0283f5-4a0f-401e-9103-751d617b59d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs_10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "txt = \"./checkpoint/2023_11_09-09_56_42-data_RS-ALL-ratio_0.1-model_RN50-method_base-kw_none-AL_False-PL_None-vit_False-epochs_10-lr_0.0005-bs_64-fold_1\"\n",
    "x = re.search(\"epochs_[0-9]+\", txt)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4b79f-34b8-43ce-b5a0-e4cd20d58d28",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21839599-a8c4-4936-8122-039e0b063b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = False\n",
    "\n",
    "if fashion:\n",
    "    str_args = '''--train-data Fashion-ALL\n",
    "            --label-ratio 0.1\n",
    "            --val-data Fashion-ALL\n",
    "            --keyword-path keywords/fashion/class-name.txt\n",
    "            --epochs 10\n",
    "            --method base  \n",
    "    '''\n",
    "else:\n",
    "    str_args = ''' --train-data RS-ALL\n",
    "            --label-ratio 0.1\n",
    "            --val-data RS-ALL\n",
    "            --imagenet-val RSICD-CLS \\\n",
    "            --keyword-path keywords/RS/class-name.txt\n",
    "            --epochs 25\n",
    "            --lr 5e-5\n",
    "            --zeroshot-frequency 5  \n",
    "            --method base\n",
    "    '''\n",
    "           # --active-learning\n",
    "        # --al-iter 3\n",
    "\n",
    "# Convert string arguments to a format that can be parsed by parse_args             \n",
    "str_args = prep_str_args(str_args)\n",
    "args = parse_args(str_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e9538-7246-4ed0-986f-42336bf8b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# checkpoint_path = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5d79c-3fbb-4e0b-996a-5c2556d12122",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1218276-2fe6-40ec-a825-31cc54c0027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# evaluate(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff853d9-f270-4443-9e6f-a98def1b60e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
