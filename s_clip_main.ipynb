{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9517c26a-cecc-4363-97d4-4538aeacb558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nhollain/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nhollain/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA RTX A6000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA RTX A6000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA RTX A5000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA RTX A5000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/vol/tensusers4/nhollain/thesis2023-2024/s_clip_scripts')\n",
    "\n",
    "import itertools\n",
    "from main import main, format_checkpoint\n",
    "from params import parse_args\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tuning_tools import prep_str_args, evaluate_checkpoint\n",
    "    \n",
    "# def evaluate(checkpoint, epoch = None, kfold = -1, split = 'val'):\n",
    "#     # print('=> Resuming checkpoint {} (epoch {})'.format(checkpoint, epoch))\n",
    "#     checkpoint = checkpoint_path \n",
    "#     if 'Fashion' in checkpoint:\n",
    "#         zeroshot_datasets = [\"Fashion200k-SUBCLS\", \"Fashion200k-CLS\", \"FashionGen-CLS\", \"FashionGen-SUBCLS\", \"Polyvore-CLS\", ]\n",
    "#         retrieval_datasets = [\"FashionGen\", \"Polyvore\", \"Fashion200k\",]\n",
    "#     else:\n",
    "#         zeroshot_datasets = [\"RSICD-CLS\", \"UCM-CLS\"] # \"WHU-RS19\", \"RSSCN7\", \"AID\" -> NOT WORKING bc of different data-loading workings\n",
    "#         if split == 'test': # Test split includes other remote sensing dataset\n",
    "#             zeroshot_datasets += [\"WHU-RS19\", \"RSSCN7\", \"AID\", \"RESISC45\"]\n",
    "#         retrieval_datasets = [\"RSICD\", \"UCM\", \"Sydney\"]\n",
    "    \n",
    "#     for dataset in zeroshot_datasets:\n",
    "#         str_args = ['--name', checkpoint, f'--imagenet-{split}', dataset, '--resume-epoch', str(epoch), '--k-fold', str(kfold)]\n",
    "#         args = parse_args(str_args)\n",
    "#         main(args)\n",
    "    \n",
    "#     for dataset in retrieval_datasets:\n",
    "#         str_args = ['--name', checkpoint, f'--{split}-data', dataset, '--resume-epoch', str(epoch), '--k-fold', str(kfold)]\n",
    "#         args = parse_args(str_args)\n",
    "#         main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fadeac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/tuning_tools.py:33\u001b[0m, in \u001b[0;36mevaluate_checkpoint\u001b[0;34m(checkpoint_path, epoch, kfold, split, dataset)\u001b[0m\n\u001b[1;32m     31\u001b[0m         lst_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--name\u001b[39m\u001b[38;5;124m'\u001b[39m, checkpoint]\n\u001b[1;32m     32\u001b[0m     args \u001b[38;5;241m=\u001b[39m parse_args(lst_args)\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m retrieval_datasets:\n\u001b[1;32m     36\u001b[0m     lst_args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-data\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--resume-epoch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(epoch), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--k-fold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(kfold)]\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/s_clip_scripts/main.py:153\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    150\u001b[0m log_base_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mlogs, args\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    152\u001b[0m random_seed(args\u001b[38;5;241m.\u001b[39mseed, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m model, preprocess_train, preprocess_val \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43maug_cfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maug_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m model \u001b[38;5;241m=\u001b[39m create_custom_model(args, model)  \u001b[38;5;66;03m# use custom model\u001b[39;00m\n\u001b[1;32m    159\u001b[0m random_seed(args\u001b[38;5;241m.\u001b[39mseed, args\u001b[38;5;241m.\u001b[39mrank)\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/open_clip/factory.py:382\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, **model_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[1;32m    359\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    360\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    378\u001b[0m ):\n\u001b[1;32m    379\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[1;32m    380\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[0;32m--> 382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[1;32m    402\u001b[0m     preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[1;32m    403\u001b[0m         pp_cfg,\n\u001b[1;32m    404\u001b[0m         is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    405\u001b[0m         aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[1;32m    406\u001b[0m     )\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/open_clip/factory.py:201\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained \u001b[38;5;129;01mand\u001b[39;00m pretrained\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    200\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading pretrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from OpenAI.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_openai_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     model_cfg \u001b[38;5;241m=\u001b[39m model_cfg \u001b[38;5;129;01mor\u001b[39;00m get_model_config(model_name)\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/open_clip/openai.py:82\u001b[0m, in \u001b[0;36mload_openai_model\u001b[0;34m(name, precision, device, cache_dir)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# FIXME support pure fp16/bf16 precision modes\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# for bf16, convert back to low-precision\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         convert_weights_to_lp(model, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:743\u001b[0m, in \u001b[0;36mModule.float\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    735\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/vol/tensusers4/nhollain/thesis2023-2024/thesis_env/lib/python3.10/site-packages/torch/nn/modules/module.py:743\u001b[0m, in \u001b[0;36mModule.float.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    735\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To evaluate CLIP without fine-tuning it!\n",
    "evaluate_checkpoint(checkpoint_path = None, epoch = 0, split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99e8109-1b98-4d55-96b6-ba9d06db445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "results = []\n",
    "if os.path.exists('./eval.txt'):   \n",
    "    with open('./eval.txt', 'r') as f:\n",
    "        results = f.readlines()\n",
    "\n",
    "# Remove any non-result lines from the eval file, and split the lines on the tab character\n",
    "# (results have format: model_name\\tdataset_name\\tmetric_name\\tmetric_value)\n",
    "results = [r.replace('\\n','').split('\\t')[0] for r in results if '\\t' in r]\n",
    "model_names = results\n",
    "# Remove the timestamp from the model names, as well as the specific fold - rest of the name contains params\n",
    "model_names = ['-'.join(m.split('-')[2:]).split('-fold')[0] for m in model_names]\n",
    "model_names = dict(Counter(model_names))\n",
    "# Show which models are already present in eval.txt\n",
    "# model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6311e1-c938-434d-9d14-04385c1347b0",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7eaa23-316d-4c09-847e-bdd9c30550a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of configs: 6\n"
     ]
    }
   ],
   "source": [
    "# Do a grid search on the parameters\n",
    "# NOTE: for active learning, save-freq should be set to 1\n",
    "base_str_args = ''' --train-data RS-ALL\n",
    "--val-data RS-ALL\n",
    "--imagenet-val RSICD-CLS \n",
    "--keyword-path keywords/RS/class-name.txt\n",
    "--zeroshot-frequency 5  \n",
    "--method ours\n",
    "--lr 5e-5\n",
    "--save-freq 1\n",
    "'''\n",
    "# --label-ratio 0.1\n",
    "# --active-learning\n",
    "\n",
    "# Dictionary of values to gridsearch for hyperparam tuning\n",
    "gridsearch_dict = {\n",
    "    '--epochs' : [25], #list(range(15,36,5)) if 'active-learning' in base_str_args else [35], #[10,15,20,25,30,35],\n",
    "    '--batch-size' : [64],\n",
    "    '--al-iter': [1], #list(range(3,17,2)), #list(range(1,6,2)),\n",
    "    '--al-epochs': [35],\n",
    "    '--label-ratio': [0.05, 0.1, 0.2, 0.4, 0.8, 1.0],\n",
    "    '--pl-method': ['hard.text'],\n",
    "}\n",
    "\n",
    "# This number is very specifically chosen because we have 9 folds for the datasets!\n",
    "num_repeats = 9\n",
    "num_evals = 20 # How many evaluations are done with evaluate(...) - KEEP THIS FIXED\n",
    "\n",
    "gridsearch_values = list(gridsearch_dict.values())\n",
    "gridsearch_keys = list(gridsearch_dict.keys())\n",
    "configs = list(itertools.product(*gridsearch_values))\n",
    "print('Number of configs:', len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405aaa0-49e5-47ad-9ec2-0dadb0ae4522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--train-data', 'RS-ALL', '--val-data', 'RS-ALL', '--imagenet-val', 'RSICD-CLS', '--keyword-path', 'keywords/RS/class-name.txt', '--zeroshot-frequency', '5', '--method', 'ours', '--lr', '5e-5', '--save-freq', '1', '--epochs', '25', '--batch-size', '64', '--al-iter', '1', '--al-epochs', '35', '--label-ratio', '0.05', '--pl-method', 'hard.text']\n",
      "Config number 0: 9 repeats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [13:37<00:00, 32.69s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [14:21<00:00, 34.47s/it]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for c, config in enumerate(configs): # Gridsearch\n",
    "    str_args = copy.deepcopy(base_str_args)\n",
    "    # Add the gridsearch parameters to the arguments\n",
    "    for i, param in enumerate(config):\n",
    "        param_name = gridsearch_keys[i]\n",
    "        str_args += '\\n{} {}'.format(param_name, param)\n",
    "        \n",
    "    str_args = prep_str_args(str_args)\n",
    "    print(str_args)\n",
    "    args = parse_args(str_args)\n",
    "    checkpoint_hypothetical = format_checkpoint(args)\n",
    "    # Remove the timestamp from the hypothetical checkpoint, so we can compare to the params of other checkpoints\n",
    "    checkpoint_params = '-'.join(checkpoint_hypothetical.split('-')[2:]).split('-fold')[0]\n",
    "\n",
    "    # Check if we've already trained the exact same model, correct the number of training iterations we still need to do\n",
    "    if checkpoint_params in model_names:\n",
    "        # The number of times to repeat depends on how often the model's been evaluated already\n",
    "        start_repeat = int(model_names[checkpoint_params]/num_evals)\n",
    "    else: # If we've never trained + evaluated the model before, just use num_repeats\n",
    "        start_repeat = 0\n",
    "    print('Config number {}: {} repeats'.format(c, max(0,num_repeats-start_repeat)))\n",
    "    for i in range(start_repeat, num_repeats):\n",
    "        # print('repeat number', i) \n",
    "        args = parse_args(str_args)\n",
    "        args.k_fold = i\n",
    "        # We compute here for which epochs we need to evaluate (based on for which epochs we checkpoint)\n",
    "        epoch_freq = args.save_freq\n",
    "        epochs = list(range(epoch_freq,args.epochs+1,epoch_freq))\n",
    "        # print('Epochs to checkpoint', epochs)\n",
    "        # print('Args k fold (outside):' , args.k_fold)\n",
    "        checkpoint_path = main(args) # Calls the main.py function of S-CLIP\n",
    "        for epoch in epochs:\n",
    "            evaluate(checkpoint_path, epoch = epoch, kfold = i, split = 'test')\n",
    "        # Remove the checkpoint after evaluating, to save space\n",
    "        os.system(\"rm -r {}\".format(checkpoint_path))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4b79f-34b8-43ce-b5a0-e4cd20d58d28",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21839599-a8c4-4936-8122-039e0b063b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = False\n",
    "\n",
    "if fashion:\n",
    "    str_args = '''--train-data Fashion-ALL\n",
    "            --label-ratio 0.1\n",
    "            --val-data Fashion-ALL\n",
    "            --keyword-path keywords/fashion/class-name.txt\n",
    "            --epochs 10\n",
    "            --method base  \n",
    "    '''\n",
    "else:\n",
    "    str_args = ''' --train-data RS-ALL\n",
    "            --label-ratio 0.1\n",
    "            --val-data RS-ALL\n",
    "            --imagenet-val RSICD-CLS \\\n",
    "            --keyword-path keywords/RS/class-name.txt\n",
    "            --epochs 5\n",
    "            --lr 5e-5\n",
    "            --zeroshot-frequency 5  \n",
    "            --method base\n",
    "            --active-learning\n",
    "            --al-iter 3\n",
    "    '''\n",
    "           # --active-learning\n",
    "        # --al-iter 3\n",
    "\n",
    "# Convert string arguments to a format that can be parsed by parse_args             \n",
    "str_args = prep_str_args(str_args)\n",
    "args = parse_args(str_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e9538-7246-4ed0-986f-42336bf8b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# checkpoint_path = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5d79c-3fbb-4e0b-996a-5c2556d12122",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1218276-2fe6-40ec-a825-31cc54c0027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# evaluate(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff853d9-f270-4443-9e6f-a98def1b60e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
