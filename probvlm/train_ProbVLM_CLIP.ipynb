{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2986807e-7850-404f-a957-eaeb16371d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/tensusers4/nhollain/ProbVLM/src/train_probVLM.py:32: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  def train_ProbVLM(CLIP_Net, BayesCap_Net, train_loader, eval_loader, Cri = TempCombLoss(), device='cuda', dtype=torch.cuda.FloatTensor(),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from os.path import join as ospj\n",
    "from os.path import expanduser\n",
    "from munch import Munch as mch\n",
    "import numpy as np\n",
    "\n",
    "from ds import prepare_coco_dataloaders, prepare_flickr_dataloaders, prepare_cub_dataloaders, prepare_flo_dataloaders, build_vocab\n",
    "\n",
    "from utils import *\n",
    "from networks import *\n",
    "from train_probVLM import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa3424d-a623-4dc5-b468-4f763f27d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probvlm_data_loader import get_data as probvlm_get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df717d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = build_vocab('/vol/tensusers5/nhollain/ProbVLM/', data_name = 'coco', \n",
    "#                     jsons = {'coco': ['captions_train2014.json', 'captions_val2014.json']}, threshold=0)\n",
    "# vocab = build_vocab('/vol/tensusers5/nhollain/ProbVLM/coco', threshold = 0)\n",
    "# vocab\n",
    "\n",
    "# import pickle\n",
    "# with open('/vol/tensusers5/nhollain/ProbVLM/src/ds/vocabs/coco_vocab.pkl', 'wb') as f:\n",
    "#     pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143d187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "sys.path.append('/vol/tensusers4/nhollain/thesis2023-2024/s_clip_scripts')\n",
    "# from data_loader import get_data\n",
    "from params import parse_args\n",
    "from open_clip import create_model_and_transforms, get_tokenizer, create_loss\n",
    "\n",
    "def prep_str_args(str_args): # Code to parse the string style arguments, as shown below\n",
    "    str_args = str_args.split('\\n') # Split on newline\n",
    "    str_args = [s.strip() for s in str_args] # Remove any whitespaces from the start and end of the strings\n",
    "    # Split on the space between the parameter name and the value, e.g. '--name x' becomes ['--name', 'x']\n",
    "    str_args = [s.split(' ') for s in str_args] \n",
    "    str_args = list(itertools.chain(*str_args)) # Flatten the resulting list of lists\n",
    "    str_args = [s for s in str_args if len(s) > 0] # Remove arguments that are empty\n",
    "    return str_args\n",
    "\n",
    "str_args = ''' \n",
    "    --train-data RS-ALL\n",
    "    --val-data RS-ALL\n",
    "    --imagenet-val RSICD-CLS \\\n",
    "    '''\n",
    "\n",
    "str_args = prep_str_args(str_args)\n",
    "args = parse_args(str_args)\n",
    "\n",
    "\n",
    "model, preprocess_train, preprocess_val = create_model_and_transforms(\n",
    "        args.model, args.pretrained, precision=args.precision, device=args.device, output_dict=True,\n",
    "        aug_cfg = args.aug_cfg, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c9e34b-6067-4cb7-be7f-655abc2cf098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coco data (split: val)\tCoco data (split: train)\t"
     ]
    }
   ],
   "source": [
    "probvlm_data = probvlm_get_data((preprocess_train, preprocess_val), tokenizer=get_tokenizer(args.model), model = model, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c94bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_data(args, (preprocess_train, preprocess_val), iter=0, tokenizer=get_tokenizer(args.model), model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d8e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cub_train_loader, cub_valid_loader = probvlm_data['train'].dataloader, probvlm_data['val'].dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "102d26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs_train_loader, rs_valid_loader = data['train'].dataloader, data['val'].dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbdb7f39-293e-48fe-bd3b-da616157f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'coco' # CUB' # coco or flickr\n",
    "# data_dir = ospj('/vol/tensusers5/nhollain/ProbVLM/', dataset) # e.g. ospj(expanduser('~'), 'Documents', 'jm', 'data', dataset)\n",
    "# dataloader_config = mch({\n",
    "#     'batch_size': 64,\n",
    "#     'random_erasing_prob': 0.,\n",
    "#     'traindata_shuffle': True\n",
    "# })\n",
    "# loaders,vocab = load_data_loader(dataset, data_dir, dataloader_config)\n",
    "# cub_train_loader, cub_valid_loader, cub_test_loader = loaders['train'], loaders['val'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da29997-5483-4fa0-b927-74b38dc36cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_net = load_model('cuda')\n",
    "CLIP_Net = load_model(device='cuda', model_path=None)\n",
    "ProbVLM_Net = BayesCap_for_CLIP(inp_dim=512, out_dim=512, hid_dim=256, num_layers=3, p_drop=0.05,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1513f40-28d1-4d4d-8d9b-e113c8cd4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> no checkpoint found at ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|██████▍                                                                                                                         | 4/80 [03:01<52:26, 41.40s/batch, loss=10.1]"
     ]
    }
   ],
   "source": [
    "train_ProbVLM(CLIP_Net, ProbVLM_Net, cub_train_loader, cub_valid_loader, Cri = TempCombLoss(), device='cuda', dtype=torch.cuda.FloatTensor,\n",
    "    init_lr=8e-5, num_epochs=10, eval_every=5, ckpt_path='../ckpt/ProbVLM_Net', T1=1e0, T2=1e-4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c97458-c611-41f8-867b-5d19ae0c71b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
