{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354a0fd6-cd04-4a6d-aeea-10362fa90b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# Based on : https://gist.github.com/soruly/bd02a218690fe4e19295de3f5bede242\n",
    "def compare(img1_path, img2_path):\n",
    "    # Read in images\n",
    "    img1, img2 = cv2.imread(img1_path), cv2.imread(img2_path)                \n",
    "    # https://stackoverflow.com/questions/65541488/dead-kernel-sift-detectgray-none-the-kernel-appears-to-have-died-it-will-re\n",
    "    # Using cv2.SIFT_create() to avoid kernel dying, like it does with cv2.SIFT()\n",
    "    sift = cv2.SIFT_create() # Initializing SIFT\n",
    "\n",
    "    # Detect keypoints and descriptors\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    # Match features between the two images\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.match(des1,des2)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f78227b-7eea-4817-92d6-597380f01aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1754"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = compare('./data/test/dog1.jpg', './data/test/dog2.jpg')\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a0751c-5cbf-4b6b-a25d-fd13cc7f5e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106303"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = compare('./data/test/cat1.jpg', './data/test/dog2.jpg')\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0e52be-77e4-43c5-a9ed-e8973a80ec6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106303"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = compare('./data/test/cat1.jpg', './data/test/dog1.jpg')\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87b372c-6574-4cf2-a0ee-9bfb6bab681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/tensusers5/nhollain/thesis_env/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/blog/image-similarity\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoFeatureExtractor\n",
    "\n",
    "model_ckpt = 'google/vit-base-patch16-224-in21k'# \"nateraw/vit-base-beans\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "hidden_dim = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9419e49-92df-492f-86b5-5e05bc2bd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"beans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3447893f-4bf2-4fb7-a085-704577912a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "seed = 42\n",
    "candidate_subset = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a02da3-1291-4bd6-9d2e-3ed940a49082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Data transformation chain.\n",
    "transformation_chain = T.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256 and then we take center crop.\n",
    "        T.Resize(int((256 / 224) * extractor.size[\"height\"])),\n",
    "        T.CenterCrop(extractor.size[\"height\"]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def extract_embeddings(model: torch.nn.Module):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]\n",
    "        # `transformation_chain` is a compostion of preprocessing\n",
    "        # transformations we apply to the input images to prepare them\n",
    "        # for the model. For more details, check out the accompanying Colab Notebook.\n",
    "        image_batch_transformed = torch.stack(\n",
    "            [transformation_chain(image) for image in images]\n",
    "        )\n",
    "        new_batch = {\"pixel_values\": image_batch_transformed.to(device)}\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "        return {\"embeddings\": embeddings}\n",
    "\n",
    "    return pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e419b3-3aab-4362-9486-eba6ba7f4ebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m extract_fn \u001b[38;5;241m=\u001b[39m extract_embeddings(model\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m----> 3\u001b[0m candidate_subset_emb \u001b[38;5;241m=\u001b[39m candidate_subset\u001b[38;5;241m.\u001b[39mmap(extract_fn, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mbatch_size\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "extract_fn = extract_embeddings(model.to(device))\n",
    "candidate_subset_emb = candidate_subset.map(extract_fn, batched=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60a6d3-658a-4614-822a-38ddbc762ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "candidate_ids = []\n",
    "\n",
    "for id in tqdm(range(len(candidate_subset_emb))):\n",
    "    label = candidate_subset_emb[id][\"labels\"]\n",
    "\n",
    "    # Create a unique indentifier.\n",
    "    entry = str(id) + \"_\" + str(label)\n",
    "\n",
    "    candidate_ids.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203289e0-5286-46c4-9ea7-fcc43dcd40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_candidate_embeddings = np.array(candidate_subset_emb[\"embeddings\"])\n",
    "all_candidate_embeddings = torch.from_numpy(all_candidate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72f560-9135-4210-9d25-59bd8e5f3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(emb_one, emb_two):\n",
    "    \"\"\"Computes cosine similarity between two vectors.\"\"\"\n",
    "    scores = torch.nn.functional.cosine_similarity(emb_one, emb_two)\n",
    "    return scores.numpy().tolist()\n",
    "\n",
    "\n",
    "def fetch_similar(image, top_k=5):\n",
    "    \"\"\"Fetches the `top_k` similar images with `image` as the query.\"\"\"\n",
    "    # Prepare the input query image for embedding computation.\n",
    "    image_transformed = transformation_chain(image).unsqueeze(0)\n",
    "    new_batch = {\"pixel_values\": image_transformed.to(device)}\n",
    "\n",
    "    # Comute the embedding.\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "\n",
    "    # Compute similarity scores with all the candidate images at one go.\n",
    "    # We also create a mapping between the candidate image identifiers\n",
    "    # and their similarity scores with the query image.\n",
    "    sim_scores = compute_scores(all_candidate_embeddings, query_embeddings)\n",
    "    similarity_mapping = dict(zip(candidate_ids, sim_scores))\n",
    " \n",
    "    # Sort the mapping dictionary and return `top_k` candidates.\n",
    "    similarity_mapping_sorted = dict(\n",
    "        sorted(similarity_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "    id_entries = list(similarity_mapping_sorted.keys())[:top_k]\n",
    "\n",
    "    ids = list(map(lambda x: int(x.split(\"_\")[0]), id_entries))\n",
    "    labels = list(map(lambda x: int(x.split(\"_\")[-1]), id_entries))\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40371c52-f79b-4c5a-a992-e7eee626b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.random.choice(len(dataset[\"test\"]))\n",
    "test_sample = dataset[\"test\"][test_idx][\"image\"]\n",
    "test_label = dataset[\"test\"][test_idx][\"labels\"]\n",
    "\n",
    "sim_ids, sim_labels = fetch_similar(test_sample)\n",
    "print(f\"Query label: {test_label}\")\n",
    "print(f\"Top 5 candidate labels: {sim_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69491818-8946-4540-ae1d-021807cc8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ids, sim_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a1ba5-aacf-486a-8f54-5c4d5ac365bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"labels\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be5da5-4dd8-47c4-9067-8068e4395c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_images(images, labels):\n",
    "    if not isinstance(labels, list):\n",
    "        labels = labels.tolist()\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    columns = 6\n",
    "    for (i, image) in enumerate(images):\n",
    "        label_id = int(labels[i])\n",
    "        ax = plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Query Image\\n\" + \"Label: {}\".format(id2label[label_id]))\n",
    "        else:\n",
    "            ax.set_title(\n",
    "                \"Similar Image # \" + str(i) + \"\\nLabel: {}\".format(id2label[label_id])\n",
    "            )\n",
    "        plt.imshow(np.array(image).astype(\"int\"))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for id, label in zip(sim_ids, sim_labels):\n",
    "    images.append(candidate_subset_emb[id][\"image\"])\n",
    "    labels.append(candidate_subset_emb[id][\"labels\"])\n",
    "\n",
    "images.insert(0, test_sample)\n",
    "labels.insert(0, test_label)\n",
    "plot_images(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b317aa-80a5-4044-981b-8220b332fa51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
