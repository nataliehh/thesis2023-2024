{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b96e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Laptop of\n",
      "[nltk_data]     Natalie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Laptop of\n",
      "[nltk_data]     Natalie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/vol/tensusers4/nhollain/thesis2023-2024/s_clip_scripts')\n",
    "sys.path.append('./scripts')\n",
    "sys.path.append('./probvlm')\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from main import main, format_checkpoint\n",
    "from params import parse_args\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from tuning_tools import prep_str_args, evaluate_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad030861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of configs: 5\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Count the models that have been trained and evaluated already, based on their parameters \n",
    "results = []\n",
    "if os.path.exists('./results/test_eval_ilt.txt'):   \n",
    "    with open('./results/test_eval_ilt.txt', 'r') as f:\n",
    "        results = f.readlines()\n",
    "\n",
    "# Remove any non-result lines from the eval file, and split the lines on the tab character\n",
    "# (results have format: model_name\\tdataset_name\\tmetric_name\\tmetric_value)\n",
    "results = [r.replace('\\n','').split('\\t')[0] for r in results if '\\t' in r]\n",
    "model_names = results\n",
    "# Remove the timestamp from the model names, as well as the specific fold - rest of the name contains params\n",
    "model_names = ['-'.join(m.split('-')[2:]).split('-fold')[0] for m in model_names]\n",
    "model_names = dict(Counter(model_names))\n",
    "# print('Model_names', model_names)\n",
    "\n",
    "# Do a grid search on the parameters\n",
    "# NOTE: for active learning, save-freq should be set to 1\n",
    "base_str_args = ''' --train-data ILT\n",
    "--test-data ILT\n",
    "--keyword-path keywords/RS/class-name.txt\n",
    "--zeroshot-frequency 5  \n",
    "--method base\n",
    "--lr 5e-5\n",
    "--eval-file ./results/eval_ilt.txt\n",
    "'''\n",
    "# --active-learning\n",
    "# --probvlm\n",
    "#--save-freq 1\n",
    "\n",
    "# Dictionary of values to gridsearch for hyperparam tuning\n",
    "gridsearch_dict = {\n",
    "    '--epochs' : [5], #list(range(15,36,5)) if 'active-learning' in base_str_args else [35], #[10,15,20,25,30,35],\n",
    "    '--batch-size' : [64],\n",
    "    '--al-iter': [5], #list(range(3,17,2)), #list(range(1,6,2)),\n",
    "    '--al-epochs': [10],\n",
    "    '--label-ratio': [0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    #'--pl-method': ['soft.text'],\n",
    "}\n",
    "\n",
    "# How many times to re-evaluate the model on the test set (to get an average and std of the results)\n",
    "num_repeats = 5\n",
    "num_evals = 20 # How many evaluations are done with evaluate_checkpoint(...) - KEEP THIS FIXED\n",
    "\n",
    "gridsearch_values = list(gridsearch_dict.values())\n",
    "gridsearch_keys = list(gridsearch_dict.keys())\n",
    "configs = list(itertools.product(*gridsearch_values))\n",
    "print('Number of configs:', len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b4bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--train-data', 'ILT', '--test-data', 'ILT', '--keyword-path', 'keywords/RS/class-name.txt', '--zeroshot-frequency', '5', '--method', 'base', '--lr', '5e-5', '--eval-file', './results/eval_ilt.txt', '--epochs', '5', '--batch-size', '64', '--al-iter', '5', '--al-epochs', '10', '--label-ratio', '0.05']\n",
      "Config number 0: 5 repeats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m main(args) \u001b[38;5;66;03m# Calls the main.py function of S-CLIP\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs:\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mevaluate_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results/eval_ilt.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Remove the checkpoint after evaluating, to save space\u001b[39;00m\n\u001b[0;32m     34\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm -r \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n",
      "File \u001b[1;32mD:\\Jupyter_notebook\\Thesis2023-2024_notebook\\tuning_tools.py:38\u001b[0m, in \u001b[0;36mevaluate_checkpoint\u001b[1;34m(checkpoint_path, epoch, kfold, split, dataset, eval_file)\u001b[0m\n\u001b[0;32m     36\u001b[0m         lst_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--eval-file\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_file]\n\u001b[0;32m     37\u001b[0m     args \u001b[38;5;241m=\u001b[39m parse_args(lst_args)\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m retrieval_datasets:\n\u001b[0;32m     41\u001b[0m     lst_args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-data\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--resume-epoch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(epoch), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--k-fold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(kfold)]\n",
      "File \u001b[1;32mD:\\Jupyter_notebook\\Thesis2023-2024_notebook\\./scripts\\main.py:154\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    151\u001b[0m log_base_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mlogs, args\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    153\u001b[0m random_seed(args\u001b[38;5;241m.\u001b[39mseed, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 154\u001b[0m model, preprocess_train, preprocess_val \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43maug_cfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maug_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m model \u001b[38;5;241m=\u001b[39m create_custom_model(args, model)  \u001b[38;5;66;03m# use custom model\u001b[39;00m\n\u001b[0;32m    160\u001b[0m random_seed(args\u001b[38;5;241m.\u001b[39mseed, args\u001b[38;5;241m.\u001b[39mrank)\n",
      "File \u001b[1;32md:\\program files (x86)\\python39\\lib\\site-packages\\open_clip\\factory.py:382\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, **model_kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[0;32m    359\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    360\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    378\u001b[0m ):\n\u001b[0;32m    379\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[0;32m    380\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[1;32m--> 382\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_model(\n\u001b[0;32m    383\u001b[0m         model_name,\n\u001b[0;32m    384\u001b[0m         pretrained,\n\u001b[0;32m    385\u001b[0m         precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[0;32m    386\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    387\u001b[0m         jit\u001b[38;5;241m=\u001b[39mjit,\n\u001b[0;32m    388\u001b[0m         force_quick_gelu\u001b[38;5;241m=\u001b[39mforce_quick_gelu,\n\u001b[0;32m    389\u001b[0m         force_custom_text\u001b[38;5;241m=\u001b[39mforce_custom_text,\n\u001b[0;32m    390\u001b[0m         force_patch_dropout\u001b[38;5;241m=\u001b[39mforce_patch_dropout,\n\u001b[0;32m    391\u001b[0m         force_image_size\u001b[38;5;241m=\u001b[39mforce_image_size,\n\u001b[0;32m    392\u001b[0m         force_preprocess_cfg\u001b[38;5;241m=\u001b[39mforce_preprocess_cfg,\n\u001b[0;32m    393\u001b[0m         pretrained_image\u001b[38;5;241m=\u001b[39mpretrained_image,\n\u001b[0;32m    394\u001b[0m         pretrained_hf\u001b[38;5;241m=\u001b[39mpretrained_hf,\n\u001b[0;32m    395\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    396\u001b[0m         output_dict\u001b[38;5;241m=\u001b[39moutput_dict,\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[0;32m    400\u001b[0m     pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[0;32m    402\u001b[0m     preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[0;32m    403\u001b[0m         pp_cfg,\n\u001b[0;32m    404\u001b[0m         is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    405\u001b[0m         aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[0;32m    406\u001b[0m     )\n",
      "File \u001b[1;32md:\\program files (x86)\\python39\\lib\\site-packages\\open_clip\\factory.py:201\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained \u001b[38;5;129;01mand\u001b[39;00m pretrained\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    200\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading pretrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from OpenAI.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 201\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_openai_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     model_cfg \u001b[38;5;241m=\u001b[39m model_cfg \u001b[38;5;129;01mor\u001b[39;00m get_model_config(model_name)\n",
      "File \u001b[1;32md:\\program files (x86)\\python39\\lib\\site-packages\\open_clip\\openai.py:64\u001b[0m, in \u001b[0;36mload_openai_model\u001b[1;34m(name, precision, device, cache_dir)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found; available models = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_openai_models()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# loading JIT archive\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     65\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# loading saved state dict\u001b[39;00m\n",
      "File \u001b[1;32md:\\program files (x86)\\python39\\lib\\site-packages\\torch\\jit\\_serialization.py:169\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, _extra_files, _restore_shapes)\u001b[0m\n\u001b[0;32m    164\u001b[0m     cpp_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mimport_ir_module_from_buffer(\n\u001b[0;32m    165\u001b[0m         cu, f\u001b[38;5;241m.\u001b[39mread(), map_location, _extra_files, _restore_shapes\n\u001b[0;32m    166\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# TODO: Pretty sure this approach loses ConstSequential status and such\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_cpp_module\u001b[49m(cpp_module)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_start = datetime.now() \n",
    "for c, config in enumerate(configs): # Gridsearch\n",
    "    str_args = copy.deepcopy(base_str_args)\n",
    "    # Add the gridsearch parameters to the arguments\n",
    "    for i, param in enumerate(config):\n",
    "        param_name = gridsearch_keys[i]\n",
    "        str_args += '\\n{} {}'.format(param_name, param)\n",
    "        \n",
    "    str_args = prep_str_args(str_args)\n",
    "    print(str_args)\n",
    "    args = parse_args(str_args)\n",
    "    checkpoint_hypothetical = format_checkpoint(args)\n",
    "    # Remove the timestamp from the hypothetical checkpoint, so we can compare to the params of other checkpoints\n",
    "    checkpoint_params = '-'.join(checkpoint_hypothetical.split('-')[2:]).split('-fold')[0]\n",
    "\n",
    "    # Check if we've already trained the exact same model, correct the number of training iterations we still need to do\n",
    "    if checkpoint_params in model_names:\n",
    "        # The number of times to repeat depends on how often the model's been evaluated already\n",
    "        start_repeat = int(model_names[checkpoint_params]/num_evals)\n",
    "    else: # If we've never trained + evaluated the model before, just use num_repeats\n",
    "        start_repeat = 0\n",
    "    print(f'Config number {c}: {max(0,num_repeats-start_repeat)} repeats')\n",
    "    for i in range(start_repeat, num_repeats):\n",
    "        args = parse_args(str_args)\n",
    "        args.seed = i\n",
    "        # We compute here for which epochs we need to evaluate (based on for which epochs we checkpoint)\n",
    "        epoch_freq = args.save_freq\n",
    "        epochs = list(range(epoch_freq,args.epochs+1,epoch_freq))\n",
    "        \n",
    "        checkpoint_path = main(args) # Calls the main.py function of S-CLIP\n",
    "        for epoch in epochs:\n",
    "            evaluate_checkpoint(checkpoint_path, epoch = epoch, split = 'test', eval_file = './results/eval_ilt.txt')\n",
    "        # Remove the checkpoint after evaluating, to save space\n",
    "        os.system(f\"rm -r {checkpoint_path}\")  \n",
    "\n",
    "t_delta = datetime.now() - t_start   \n",
    "print(f'Elapsed time: {t_delta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f9db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
