{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e50460b-2351-45d3-8c5a-e3971b3900e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fbc8e-96a0-493d-af50-05d7848ab13a",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1ab4fe-6e21-4de5-8c34-b5ec95da823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_underscore_after(val): # Remove underscore, and keep the part after the underscore\n",
    "    return val.split('_')[-1]\n",
    "def remove_underscore_before(val): # Remove underscore, and keep the part before the underscore\n",
    "    return val.split('_')[0]\n",
    "def map2d(func, grid): # Mapping for 2d arrays, from: https://stackoverflow.com/questions/70742445/elegant-map-over-2d-list\n",
    "    return [[func(value) for value in row] for row in grid]\n",
    "def full_display(df):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(df)\n",
    "def prep_and_store_results(txt_path: str): # Prepare the (text-file) results to be stored in a csv\n",
    "    with open(txt_path, 'r') as f:\n",
    "        results = f.readlines()\n",
    "\n",
    "    # Remove any non-result lines from the eval file, and split the lines on the tab character\n",
    "    # (results have format: model_name\\tdataset_name\\tmetric_name\\tmetric_value)\n",
    "    results = [r.replace('\\n','').split('\\t') for r in results if '\\t' in r]\n",
    "\n",
    "    # Make a dataframe from the results\n",
    "    df = pd.DataFrame(results, columns = ['model', 'dataset', 'metric', 'value'])\n",
    "    df['value'] = pd.to_numeric(df['value'])\n",
    "    # Remove the timestamp from the model names\n",
    "    df['model'] = df['model'].map(lambda x: '-'.join(x.split('-')[2:]))\n",
    "\n",
    "    # Make a list of model names, split by parameters - model names look like var1_xxx-var2_yyy-var3_zzz-... so split on '-'\n",
    "    models = df['model'].str.split('-').tolist()\n",
    "    \n",
    "    # Remove all underscores from our 2d list, keep one list of the param names and one with param vals\n",
    "    model_names_list = map2d(remove_underscore_before, models) # Keep the part before the underscore, aka the variable name\n",
    "    model_names = np.array(model_names_list)\n",
    "    model_val_list = map2d(remove_underscore_after, models) # Keep the part after the underscore, aka the variable's value\n",
    "    model_vals = np.array(model_val_list)\n",
    "    \n",
    "    print('Number of evaluations:', model_vals.shape[0])\n",
    "\n",
    "    # Splitting model name into columns, using the list of variables and their values\n",
    "    for i in range(model_names.shape[-1]):\n",
    "        name = model_names[0][i]\n",
    "        val = model_vals[:,i]\n",
    "        df[name] = val\n",
    "        try: # Try to make columns numeric if possible\n",
    "            df[name] = pd.to_numeric(df[name]) \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Remove columns that aren't used\n",
    "    df = df.drop(['vit', 'model', 'data', 'ALL',  'kw'], axis = 1) #'method', 'AL.iter', 'ratio', 'PL',\n",
    "    if 'fold' in df.columns.tolist():\n",
    "        df = df.drop(['fold'], axis = 1)\n",
    "    \n",
    "    # Replace 'None' with NaN, to allow conversion to numerical\n",
    "    df['AL.iter'] = df['AL.iter'].replace('None', np.nan)\n",
    "    df['AL.iter'] = pd.to_numeric(df['AL.iter'])\n",
    "    df['AL.epochs'] = df['AL.epochs'].replace('None', np.nan)\n",
    "    df['AL.epochs'] = pd.to_numeric(df['AL.epochs'])\n",
    "\n",
    "    cols = sorted(df.columns.tolist()) # Get a list of the columns of the dataframe\n",
    "    print('Column names:', cols)\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    # Group by the model parameters and randomly X model runs to use in the analysis (5 for test, 9 for val) \n",
    "    df_grouped = df.groupby(list(set(cols)-set(['value'])), dropna = False).sample(frac = 1).head(5 if 'test' in txt_path else 9)\n",
    "    # Compute mean, std performance and number of runs for each model \n",
    "    df_grouped = df_grouped.agg({'value':['mean', 'std', 'count']})\n",
    "\n",
    "    df_grouped.to_csv(txt_path.replace('.txt', '.csv'))\n",
    "    display(df_grouped)\n",
    "    return df_grouped\n",
    "\n",
    "def get_results_per_model(df, hyperparam_tuning = True):\n",
    "    if hyperparam_tuning: # Only report on the results for a specific label ratio if we're hyperparam tuning\n",
    "        df = df[(df['ratio'] == 0.1)]\n",
    "    df_no_finetune = df[(df['epochs']==0)]\n",
    "    df_baseline = df[(df['AL.iter'].isna()) & (df['method'] == 'base') & (df['epochs'] > 0)]\n",
    "    df_S_CLIP = df[(df['AL.iter'].isna()) & (df['method'] == 'ours') & (df['PL'].str.contains('ot.'))]\n",
    "    df_soft_PL = df[(df['AL.iter'].isna()) & (df['method'] == 'ours') & (df['PL'].str.contains('soft.'))]\n",
    "    df_hard_PL = df[(df['AL.iter'].isna()) & (df['method'] == 'ours') & (df['PL'].str.contains('hard.'))]\n",
    "    df_basic_AL = df[(df['AL.iter']>0) & (df['epochs']==15)]\n",
    "    \n",
    "    return { # return a dictionary of results per model\n",
    "        'baseline-finetuned': df_baseline, 'baseline-not-finetuned' : df_no_finetune, 's-clip': df_S_CLIP, 'soft-pl': df_soft_PL, \n",
    "        'hard-pl': df_hard_PL, 'basic-al': df_basic_AL, 'probvlm': None \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa32a3d-ca4c-49bc-8541-8b274642a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547585/3290507503.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  model_names = np.array(model_names_list)\n",
      "/tmp/ipykernel_547585/3290507503.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  model_vals = np.array(model_val_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluations: 50081\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m validation_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./eval.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_grouped \u001b[38;5;241m=\u001b[39m \u001b[43mprep_and_store_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mprep_and_store_results\u001b[0;34m(txt_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model_names\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     37\u001b[0m     name \u001b[38;5;241m=\u001b[39m model_names[\u001b[38;5;241m0\u001b[39m][i]\n\u001b[0;32m---> 38\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_vals\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m     df[name] \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# Try to make columns numeric if possible\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "validation_path = './eval.txt'\n",
    "df_grouped = prep_and_store_results(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f3bf5-a7cc-412e-9cc2-7703fd8f7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_grouped.reset_index()\n",
    "df_results['AL.epochs'] = df_results['AL.epochs'].fillna(df_results['epochs'])\n",
    "df_results = df_results.sort_values(['metric', 'dataset', ('value', 'mean')])\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaecb79-e771-4c07-9f54-2484c44ac363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results, split per model (e.g. get results of baseline, s-clip, etc.)\n",
    "results_dict = get_results_per_model(df_results)\n",
    "df_baseline = results_dict['baseline-finetuned']\n",
    "df_basic_AL = results_dict['basic-al']\n",
    "df_S_CLIP = results_dict['s-clip']\n",
    "df_soft_PL = results_dict['soft-pl']\n",
    "df_hard_PL = results_dict['hard-pl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90272ea1-7d07-4c5d-9c49-af6ffe2de55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results with basic active learning\n",
    "full_display(df_basic_AL.groupby(['metric', 'dataset']).tail(3)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37381b-1a6f-46a8-b89b-566a990d4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_display(df_baseline.groupby(['metric', 'dataset']).tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510a6dc-ff7c-4fd5-9c93-675ef4fed3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_display(df_S_CLIP.groupby(['metric', 'dataset']).tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b729b5a-d976-4860-800d-133758314b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_display(df_soft_PL.groupby(['metric', 'dataset']).tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d7dfc-8f35-461d-95da-a9cba08fc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_display(df_hard_PL.groupby(['metric', 'dataset']).tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b3833-748a-496c-a30d-ebfa7cfffbf6",
   "metadata": {},
   "source": [
    "# Test split evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90870d-c226-4da5-8608-9b8f519796e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './test_eval.txt'\n",
    "df_grouped = prep_and_store_results(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6f608-977d-45ba-8ffe-198ccbf51fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_grouped.reset_index()\n",
    "df_results['AL.epochs'] = df_results['AL.epochs'].fillna(df_results['epochs'])\n",
    "df_results = df_results.sort_values(['metric', 'dataset', ('value', 'mean')])\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440a52b-955a-486b-89ef-cad397632e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results, split per model (e.g. get results of baseline, s-clip, etc.)\n",
    "results_dict = get_results_per_model(df_results, False)\n",
    "df_no_finetune = results_dict['baseline-not-finetuned']\n",
    "df_baseline = results_dict['baseline-finetuned']\n",
    "df_basic_AL = results_dict['basic-al']\n",
    "df_S_CLIP = results_dict['s-clip']\n",
    "df_soft_PL = results_dict['soft-pl']\n",
    "df_hard_PL = results_dict['hard-pl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_no_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88728328-43d1-4182-a802-7048c44b7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = df_baseline[(df_baseline['epochs']==25)&(df_baseline['bs']==64)&(df_baseline['lr']==5e-5)]\n",
    "full_display(df_baseline) # .groupby(['metric', 'dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bd917-f713-4b98-bb19-75c00587c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_per_label_ratio(df, metric, dataset):\n",
    "    df_filtered = df[(df['metric'] == metric) & (df['dataset'] == dataset)]\n",
    "    # Ensure the order is from the smallest label ratio to the largest\n",
    "    df_filtered = df_filtered.sort_values(by='ratio')\n",
    "    display(df_filtered)\n",
    "    # Get the performance mean for the metric\n",
    "    performance = df_filtered[('value', 'mean')].to_numpy()\n",
    "    label_ratios = df_filtered['ratio'].to_numpy()\n",
    "    return performance, label_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2accc1-91cd-41ae-9062-c69566156cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_model_comparison(results_dict, metric, dataset):\n",
    "    epochs = {'baseline-finetuned': 25, 'basic-al': 15, 's-clip': 25, 'soft-pl': 30, \n",
    "              'hard-pl': 25,  'baseline-not-finetuned': 0}\n",
    "    # Get the performance of each model, for the given metric and dataset\n",
    "    for model in results_dict:\n",
    "        model_results = results_dict[model]\n",
    "        # If we have any results for the given model, add it to the plot\n",
    "        if model_results is not None and model_results.shape[0] > 0: \n",
    "            # Filter for correct number of epochs\n",
    "            model_results = model_results[model_results['epochs'] == epochs[model]]\n",
    "            performance, label_ratios = performance_per_label_ratio(model_results, metric, dataset)\n",
    "            if model == 'baseline-not-finetuned': # repeat static baseline performance 5 times (once for each label ratio)\n",
    "                performance = 5 * [performance] \n",
    "            plt.plot(performance, label = model)\n",
    "    # Add information about the dataset, metric and label ratios to the plot\n",
    "    plt.title(f'{metric} (dataset: {dataset})')\n",
    "    plt.xticks(np.arange(label_ratios.shape[0]), label_ratios)\n",
    "    ylabel = 'recall' if 'R@' in metric else 'accuracy'\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('Label ratio')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e9d84-27cf-4652-9aa7-eaa55c4161b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'image_to_text_R@1'\n",
    "datasets = ['RSICD', 'UCM', 'Sydney']\n",
    "plot_model_comparison(results_dict, metric, 'RSICD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aba941-655a-4b8d-a58b-a2d42b2200ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(results_dict, metric, 'UCM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52cfb2-284d-4e78-8fbd-19806e429e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(results_dict, metric, 'Sydney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27918c43-5f90-433f-9479-a643723e86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'zeroshot-val-top1'\n",
    "datasets = [\"RSICD-CLS\", \"UCM-CLS\", \"WHU-RS19\", \"RSSCN7\", \"AID\", \"RESISC45\"]\n",
    "for dataset in datasets:\n",
    "    print(dataset, metric)\n",
    "    plot_model_comparison(results_dict, metric, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b8ed9-5e52-4646-bfac-6016eb1874ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
